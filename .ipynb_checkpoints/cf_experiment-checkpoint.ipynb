{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a889590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in ./venv/lib/python3.12/site-packages (from tensorflow) (4.14.1)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.11.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.3.2)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading keras-3.11.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.8/408.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt_einsum, ml_dtypes, mdurl, markdown, h5py, grpcio, google_pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google_pasta-0.2.0 grpcio-1.74.0 h5py-3.14.0 keras-3.11.2 libclang-18.1.1 markdown-3.8.2 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 protobuf-6.32.0 rich-14.1.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726b4c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 17:39:31.329590: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-06 17:39:31.941511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-06 17:39:33.685705: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#we need to create Y, R, Ynorm, Ymean from reviews.csv and animes.csv (num_users is number of unique profiles in reviews.csv, num_items is number of unique anime_ids in animes.csv)\n",
    "#Y matrix is ratings, R matrix is presence of ratings, Ynorm is normalized ratings, Ymean is mean ratings for each item\n",
    "#Y shape is (num_items, num_users), R shape is (num_items, num_users)\n",
    "\n",
    "animes_df = pd.read_csv(\"data/animes.csv\")\n",
    "reviews_df = pd.read_csv(\"data/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f097d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map IDs to indices\n",
    "anime_id_to_idx = {aid: idx for idx, aid in enumerate(animes_df['uid'].unique())}\n",
    "user_id_to_idx  = {uid: idx for idx, uid in enumerate(reviews_df['profile'].unique())}\n",
    "\n",
    "# Convert to indices\n",
    "reviews_df['anime_idx'] = reviews_df['anime_uid'].map(anime_id_to_idx)\n",
    "reviews_df['profile_idx']  = reviews_df['profile'].map(user_id_to_idx)\n",
    "\n",
    "# Compute mean per anime\n",
    "anime_means = reviews_df.groupby('anime_idx')['score'].mean().to_dict()\n",
    "\n",
    "num_items = len(anime_id_to_idx)\n",
    "\n",
    "anime_means_array = np.zeros(num_items)   # default fill with 0\n",
    "for idx, mean in anime_means.items():\n",
    "    anime_means_array[idx] = mean\n",
    "\n",
    "# Normalize ratings (subtract per-anime mean)\n",
    "reviews_df['score_norm'] = reviews_df.apply(\n",
    "    lambda row: row['score'] - anime_means_array[row['anime_idx']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only needed columns (triplet + normalized score)\n",
    "ratings_df = reviews_df[['anime_idx', 'profile_idx', 'score', 'score_norm']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f54a9",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# HYPERPARAMETERS - Tune these to reduce overfitting\n# =============================================================================\nnum_features = 10      # REDUCED from 20 - fewer features = simpler model\nlambda_ = 10           # INCREASED from 1 - more regularization = less overfitting\nlearning_rate = 0.1\niterations = 500       # REDUCED - we'll use early stopping anyway\n\nnum_users = len(user_id_to_idx) + 1  # +1 for the new user\n\n# Initialize parameters\ntf.random.set_seed(42)  # For reproducibility\nX = tf.Variable(tf.random.normal((num_items, num_features), dtype=tf.float64), name='X')\nW = tf.Variable(tf.random.normal((num_users, num_features), dtype=tf.float64), name='W')\nb = tf.Variable(tf.random.normal((1, num_users), dtype=tf.float64), name='b')\n\nprint(f\"Model configuration:\")\nprint(f\"  - num_features: {num_features}\")\nprint(f\"  - lambda (regularization): {lambda_}\")\nprint(f\"  - num_items: {num_items}\")\nprint(f\"  - num_users: {num_users}\")\nprint(f\"  - Total parameters: {num_items * num_features + num_users * num_features + num_users:,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c716cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate a new user's ratings to Y and R at the beginning\n",
    "new_user_id = \"new_user\"\n",
    "new_user_idx = num_users - 1\n",
    "\n",
    "new_ratings = [\n",
    "    (0, new_user_idx, 10),\n",
    "    (50, new_user_idx, 8),\n",
    "    (100, new_user_idx, 9),\n",
    "    (150, new_user_idx, 7),\n",
    "    (200, new_user_idx, 6),\n",
    "    (250, new_user_idx, 8),\n",
    "    (300, new_user_idx, 9),\n",
    "    (350, new_user_idx, 7),\n",
    "]\n",
    "\n",
    "new_df = pd.DataFrame(new_ratings, columns=['anime_idx', 'profile_idx', 'score'])\n",
    "\n",
    "# normalize new ratings\n",
    "new_df['score_norm'] = new_df.apply(\n",
    "    lambda row: row['score'] - anime_means_array[row['anime_idx']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "ratings_df = pd.concat([ratings_df, new_df], ignore_index=True)\n",
    "#change this code to simulate a new user with some ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b043572",
   "metadata": {},
   "outputs": [],
   "source": "# 1. Remove rows with NaNs in anime_idx, user_idx, or score_norm\nratings_df = ratings_df.dropna(subset=['anime_idx', 'profile_idx', 'score_norm'])\n\n# 2. Train/Test Split (80/20)\n# We shuffle and split the ratings, not users or items\n# This tests: \"Can we predict ratings we haven't seen?\"\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n\nprint(f\"Total ratings: {len(ratings_df):,}\")\nprint(f\"Training ratings: {len(train_df):,} ({len(train_df)/len(ratings_df)*100:.1f}%)\")\nprint(f\"Test ratings: {len(test_df):,} ({len(test_df)/len(ratings_df)*100:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82499670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func_triplet(X, W, b, anime_idx_tensor, user_idx_tensor, ratings_tensor, lambda_):\n",
    "    preds = tf.reduce_sum(tf.gather(X, anime_idx_tensor) * tf.gather(W, user_idx_tensor), axis=1) + tf.gather(b[0], user_idx_tensor)\n",
    "    err = preds - ratings_tensor\n",
    "    J = 0.5 * tf.reduce_sum(err**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba237c3",
   "metadata": {},
   "outputs": [],
   "source": "# Convert TRAINING data to TensorFlow tensors\nanime_idx_tensor = tf.constant(train_df['anime_idx'].values, dtype=tf.int32)\nuser_idx_tensor  = tf.constant(train_df['profile_idx'].values, dtype=tf.int32)\nratings_tensor   = tf.constant(train_df['score_norm'].values, dtype=tf.float64)\n\noptimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n\n# Helper function to calculate RMSE\ndef calculate_rmse(X, W, b, df, anime_means_array):\n    \"\"\"Calculate RMSE on a dataframe of ratings\"\"\"\n    anime_idx = df['anime_idx'].values.astype(int)\n    user_idx = df['profile_idx'].values.astype(int)\n    actual_scores = df['score'].values\n    \n    X_np = X.numpy()\n    W_np = W.numpy()\n    b_np = b.numpy()\n    \n    predictions = np.sum(X_np[anime_idx] * W_np[user_idx], axis=1) + b_np[0, user_idx] + anime_means_array[anime_idx]\n    \n    rmse = np.sqrt(np.mean((predictions - actual_scores) ** 2))\n    return rmse\n\nprint(f\"Training on {len(train_df):,} ratings...\")\nprint(f\"Hyperparameters: features={num_features}, lambda={lambda_}, lr={learning_rate}\")\nprint(\"-\" * 60)\n\n# Early stopping: track best test RMSE\nbest_test_rmse = float('inf')\nbest_iter = 0\npatience = 100  # Stop if no improvement for this many iterations\nno_improve_count = 0\n\nhistory = {'iter': [], 'train_rmse': [], 'test_rmse': []}\n\nfor iter in range(iterations):\n    with tf.GradientTape() as tape:\n        cost_value = cofi_cost_func_triplet(\n            X, W, b, anime_idx_tensor, user_idx_tensor, ratings_tensor, lambda_\n        )\n    grads = tape.gradient(cost_value, [X, W, b])\n    optimizer.apply_gradients(zip(grads, [X, W, b]))\n\n    if iter % 20 == 0:\n        train_rmse = calculate_rmse(X, W, b, train_df, anime_means_array)\n        test_rmse = calculate_rmse(X, W, b, test_df, anime_means_array)\n        \n        history['iter'].append(iter)\n        history['train_rmse'].append(train_rmse)\n        history['test_rmse'].append(test_rmse)\n        \n        # Check for improvement\n        if test_rmse < best_test_rmse:\n            best_test_rmse = test_rmse\n            best_iter = iter\n            no_improve_count = 0\n            marker = \" *best*\"\n        else:\n            no_improve_count += 20\n            marker = \"\"\n        \n        print(f\"Iter {iter:4d} | Train: {train_rmse:.4f} | Test: {test_rmse:.4f}{marker}\")\n        \n        # Early stopping check\n        if no_improve_count >= patience:\n            print(f\"\\nEarly stopping: No improvement for {patience} iterations\")\n            break\n\nprint(\"-\" * 60)\nprint(f\"Training complete!\")\ntrain_rmse = calculate_rmse(X, W, b, train_df, anime_means_array)\ntest_rmse = calculate_rmse(X, W, b, test_df, anime_means_array)\nprint(f\"Final Train RMSE: {train_rmse:.4f}\")\nprint(f\"Final Test RMSE:  {test_rmse:.4f}\")\nprint(f\"Best Test RMSE:   {best_test_rmse:.4f} (at iteration {best_iter})\")"
  },
  {
   "cell_type": "code",
   "id": "rij8ukexd6f",
   "source": "# Evaluation interpretation\nimport matplotlib.pyplot as plt\n\nprint(\"=\" * 60)\nprint(\"EVALUATION SUMMARY\")\nprint(\"=\" * 60)\n\nprint(f\"\\nHyperparameters used:\")\nprint(f\"  - num_features: {num_features}\")\nprint(f\"  - lambda (regularization): {lambda_}\")\nprint(f\"  - learning_rate: {learning_rate}\")\n\nprint(f\"\\nResults:\")\nprint(f\"  Train RMSE: {train_rmse:.4f}\")\nprint(f\"  Test RMSE:  {test_rmse:.4f}\")\nprint(f\"  Best Test:  {best_test_rmse:.4f} (iter {best_iter})\")\n\ngap = test_rmse - train_rmse\nprint(f\"\\nOverfitting gap: {gap:.4f}\")\n\nif gap > 1.0:\n    print(\"⚠️  Still overfitting. Try: higher lambda, fewer features\")\nelif gap > 0.5:\n    print(\"⚠️  Moderate overfitting. Getting better!\")\nelif gap > 0.2:\n    print(\"✓  Mild overfitting - acceptable\")\nelse:\n    print(\"✓  Good fit - train and test are close\")\n\nprint(\"\\nBenchmarks (1-10 scale):\")\nprint(\"  RMSE < 1.0: Excellent | 1.0-1.5: Good | 1.5-2.0: OK | > 2.0: Poor\")\n\n# Plot training history\nif len(history['iter']) > 1:\n    plt.figure(figsize=(10, 5))\n    plt.plot(history['iter'], history['train_rmse'], 'b-', label='Train RMSE')\n    plt.plot(history['iter'], history['test_rmse'], 'r-', label='Test RMSE')\n    plt.xlabel('Iteration')\n    plt.ylabel('RMSE')\n    plt.title('Training Progress')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b60afbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New user's actual ratings vs predicted ratings:\n",
      "Anime idx 0: Haikyuu!! Second Season, Actual = 10.0, Predicted = 9.95\n",
      "Anime idx 50: Last Exile, Actual = 8.0, Predicted = 7.91\n",
      "Anime idx 100: Higashi no Eden, Actual = 9.0, Predicted = 8.83\n",
      "Anime idx 150: Eikoku Koi Monogatari Emma: Molders-hen, Actual = 7.0, Predicted = 7.36\n",
      "Anime idx 200: Mobile Suit Zeta Gundam, Actual = 6.0, Predicted = 6.20\n",
      "Anime idx 250: Drifters, Actual = 8.0, Predicted = 7.88\n",
      "Anime idx 300: Omae Umasou da na, Actual = 9.0, Predicted = 8.74\n",
      "Anime idx 350: Toaru Kagaku no Railgun S, Actual = 7.0, Predicted = 7.12\n",
      "\n",
      "Top 10 anime recommendations for the new user:\n",
      "Anime ID 21085: Witch Craft Works (Predicted Rating: 11.43)\n",
      "Anime ID 7311: Suzumiya Haruhi no Shoushitsu (Predicted Rating: 10.92)\n",
      "Anime ID 34822: Tsuki ga Kirei (Predicted Rating: 10.91)\n",
      "Anime ID 9756: Mahou Shoujo Madoka★Magica (Predicted Rating: 10.72)\n",
      "Anime ID 1735: Naruto: Shippuuden (Predicted Rating: 10.48)\n",
      "Anime ID 5114: Fullmetal Alchemist: Brotherhood (Predicted Rating: 10.23)\n",
      "Anime ID 34599: Made in Abyss (Predicted Rating: 10.13)\n",
      "Anime ID 32: Neon Genesis Evangelion: The End of Evangelion (Predicted Rating: 10.08)\n",
      "Anime ID 205: Samurai Champloo (Predicted Rating: 10.07)\n",
      "Anime ID 239: Gankutsuou (Predicted Rating: 10.03)\n"
     ]
    }
   ],
   "source": [
    "# Get feature vector and bias for the new user\n",
    "w_new = W[new_user_idx].numpy()   # (num_features,)\n",
    "b_new = b[0, new_user_idx].numpy()  # scalar\n",
    "\n",
    "my_predictions = X.numpy().dot(w_new) + b_new + anime_means_array\n",
    "ix = np.argsort(my_predictions)[::-1]\n",
    "print(\"\\nNew user's actual ratings vs predicted ratings:\")\n",
    "for _, row in new_df.iterrows():\n",
    "    idx = int(row['anime_idx'])\n",
    "    actual = row['score']\n",
    "    predicted = my_predictions[idx]\n",
    "    \n",
    "    # get title from animes_df \n",
    "    title = animes_df['title'].iloc[idx]\n",
    "    \n",
    "    print(f\"Anime idx {idx}: {title}, Actual = {actual}, Predicted = {predicted:.2f}\")\n",
    "print(\"\\nTop 10 anime recommendations for the new user:\")\n",
    "for i in ix[:10]:\n",
    "    anime_id = animes_df['uid'].iloc[i]\n",
    "    title = animes_df['title'].iloc[i]\n",
    "    print(f\"Anime ID {anime_id}: {title} (Predicted Rating: {my_predictions[i]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d6c73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative filtering model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from save_and_load_cf_recommender import save_collab_model, load_collab_model\n",
    "save_collab_model(X, W, b, anime_means_array, anime_id_to_idx, user_id_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b77f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative filtering model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "X_loaded, W_loaded, b_loaded, anime_means_array_loaded, anime_id_to_idx_loaded, user_id_to_idx_loaded = load_collab_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9003554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify predictions are identical\n",
    "np.allclose(X.numpy(), X_loaded.numpy())\n",
    "np.allclose(W.numpy(), W_loaded.numpy())\n",
    "np.allclose(b.numpy(), b_loaded.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}