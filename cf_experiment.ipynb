{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a889590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in ./venv/lib/python3.12/site-packages (from tensorflow) (4.14.1)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.11.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.3.2)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading keras-3.11.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.8/408.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt_einsum, ml_dtypes, mdurl, markdown, h5py, grpcio, google_pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google_pasta-0.2.0 grpcio-1.74.0 h5py-3.14.0 keras-3.11.2 libclang-18.1.1 markdown-3.8.2 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 protobuf-6.32.0 rich-14.1.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726b4c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 15:46:14.307633: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-29 15:46:14.883858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-29 15:46:16.600159: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#we need to create Y, R, Ynorm, Ymean from reviews.csv and animes.csv (num_users is number of unique profiles in reviews.csv, num_items is number of unique anime_ids in animes.csv)\n",
    "#Y matrix is ratings, R matrix is presence of ratings, Ynorm is normalized ratings, Ymean is mean ratings for each item\n",
    "#Y shape is (num_items, num_users), R shape is (num_items, num_users)\n",
    "\n",
    "animes_df = pd.read_csv(\"data/animes.csv\")\n",
    "reviews_df = pd.read_csv(\"data/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f097d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map IDs to indices\n",
    "anime_id_to_idx = {aid: idx for idx, aid in enumerate(animes_df['uid'].unique())}\n",
    "user_id_to_idx  = {uid: idx for idx, uid in enumerate(reviews_df['profile'].unique())}\n",
    "\n",
    "# Convert to indices\n",
    "reviews_df['anime_idx'] = reviews_df['anime_uid'].map(anime_id_to_idx)\n",
    "reviews_df['profile_idx']  = reviews_df['profile'].map(user_id_to_idx)\n",
    "\n",
    "# Compute mean per anime\n",
    "anime_means = reviews_df.groupby('anime_idx')['score'].mean().to_dict()\n",
    "\n",
    "num_items = len(anime_id_to_idx)\n",
    "\n",
    "anime_means_array = np.zeros(num_items)   # default fill with 0\n",
    "for idx, mean in anime_means.items():\n",
    "    anime_means_array[idx] = mean\n",
    "\n",
    "# Normalize ratings (subtract per-anime mean)\n",
    "reviews_df['score_norm'] = reviews_df.apply(\n",
    "    lambda row: row['score'] - anime_means_array[row['anime_idx']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only needed columns (triplet + normalized score)\n",
    "ratings_df = reviews_df[['anime_idx', 'profile_idx', 'score', 'score_norm']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476f54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_users = len(user_id_to_idx) + 1  # +1 for the new user\n",
    "num_features = 20\n",
    "\n",
    "# Initialize parameters\n",
    "X = tf.Variable(tf.random.normal((num_items, num_features), dtype=tf.float64), name='X')\n",
    "W = tf.Variable(tf.random.normal((num_users, num_features), dtype=tf.float64), name='W')\n",
    "b = tf.Variable(tf.random.normal((1, num_users), dtype=tf.float64), name='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27c716cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate a new user's ratings to Y and R at the beginning\n",
    "new_user_id = \"new_user\"\n",
    "new_user_idx = num_users - 1\n",
    "\n",
    "new_ratings = [\n",
    "    (0, new_user_idx, 10),\n",
    "    (50, new_user_idx, 8),\n",
    "    (100, new_user_idx, 9),\n",
    "    (150, new_user_idx, 7),\n",
    "    (200, new_user_idx, 6),\n",
    "    (250, new_user_idx, 8),\n",
    "    (300, new_user_idx, 9),\n",
    "    (350, new_user_idx, 7),\n",
    "]\n",
    "\n",
    "new_df = pd.DataFrame(new_ratings, columns=['anime_idx', 'profile_idx', 'score'])\n",
    "\n",
    "# normalize new ratings\n",
    "new_df['score_norm'] = new_df.apply(\n",
    "    lambda row: row['score'] - anime_means_array[row['anime_idx']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "ratings_df = pd.concat([ratings_df, new_df], ignore_index=True)\n",
    "#change this code to simulate a new user with some ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b043572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove rows with NaNs in anime_idx, user_idx, or score_norm\n",
    "ratings_df = ratings_df.dropna(subset=['anime_idx', 'profile_idx', 'score_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82499670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func_triplet(X, W, b, anime_idx_tensor, user_idx_tensor, ratings_tensor, lambda_):\n",
    "    preds = tf.reduce_sum(tf.gather(X, anime_idx_tensor) * tf.gather(W, user_idx_tensor), axis=1) + tf.gather(b[0], user_idx_tensor)\n",
    "    err = preds - ratings_tensor\n",
    "    J = 0.5 * tf.reduce_sum(err**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bba237c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at iteration 0: 2979350.96\n",
      "Training loss at iteration 20: 227408.38\n",
      "Training loss at iteration 40: 83988.19\n",
      "Training loss at iteration 60: 43321.02\n",
      "Training loss at iteration 80: 30267.70\n",
      "Training loss at iteration 100: 24927.77\n",
      "Training loss at iteration 120: 22186.46\n",
      "Training loss at iteration 140: 20561.35\n",
      "Training loss at iteration 160: 19509.70\n",
      "Training loss at iteration 180: 18786.53\n",
      "Training loss at iteration 200: 18265.64\n",
      "Training loss at iteration 220: 17876.20\n",
      "Training loss at iteration 240: 17575.79\n",
      "Training loss at iteration 260: 17339.64\n",
      "Training loss at iteration 280: 17149.96\n",
      "Training loss at iteration 300: 16995.20\n",
      "Training loss at iteration 320: 16866.73\n",
      "Training loss at iteration 340: 16758.42\n",
      "Training loss at iteration 360: 16666.13\n",
      "Training loss at iteration 380: 16586.89\n",
      "Training loss at iteration 400: 16518.06\n",
      "Training loss at iteration 420: 16456.41\n",
      "Training loss at iteration 440: 16402.74\n",
      "Training loss at iteration 460: 16354.42\n",
      "Training loss at iteration 480: 16311.59\n",
      "Training loss at iteration 500: 16271.24\n",
      "Training loss at iteration 520: 16235.75\n",
      "Training loss at iteration 540: 16202.90\n",
      "Training loss at iteration 560: 16171.63\n",
      "Training loss at iteration 580: 16145.60\n",
      "Training loss at iteration 600: 16118.10\n",
      "Training loss at iteration 620: 16093.61\n",
      "Training loss at iteration 640: 16071.18\n",
      "Training loss at iteration 660: 16048.75\n",
      "Training loss at iteration 680: 16028.58\n",
      "Training loss at iteration 700: 16010.65\n",
      "Training loss at iteration 720: 15992.23\n",
      "Training loss at iteration 740: 15975.93\n",
      "Training loss at iteration 760: 15960.76\n",
      "Training loss at iteration 780: 15945.46\n",
      "Training loss at iteration 800: 15931.63\n",
      "Training loss at iteration 820: 15916.95\n",
      "Training loss at iteration 840: 15903.70\n",
      "Training loss at iteration 860: 15892.15\n",
      "Training loss at iteration 880: 15881.14\n",
      "Training loss at iteration 900: 15869.75\n",
      "Training loss at iteration 920: 15857.94\n",
      "Training loss at iteration 940: 15848.99\n",
      "Training loss at iteration 960: 15839.88\n",
      "Training loss at iteration 980: 15828.92\n"
     ]
    }
   ],
   "source": [
    "# Convert to TensorFlow tensors with correct dtypes\n",
    "anime_idx_tensor = tf.constant(ratings_df['anime_idx'].values, dtype=tf.int32)\n",
    "user_idx_tensor  = tf.constant(ratings_df['profile_idx'].values, dtype=tf.int32)\n",
    "ratings_tensor   = tf.constant(ratings_df['score_norm'].values, dtype=tf.float64)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.1)\n",
    "iterations = 1000\n",
    "lambda_ = 1\n",
    "\n",
    "for iter in range(iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost_value = cofi_cost_func_triplet(\n",
    "            X, W, b, anime_idx_tensor, user_idx_tensor, ratings_tensor, lambda_\n",
    "        )\n",
    "    grads = tape.gradient(cost_value, [X, W, b])\n",
    "    optimizer.apply_gradients(zip(grads, [X, W, b]))\n",
    "\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Training loss at iteration {iter}: {cost_value.numpy():0.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b60afbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New user's actual ratings vs predicted ratings:\n",
      "Anime idx 0: Haikyuu!! Second Season, Actual = 10.0, Predicted = 9.96\n",
      "Anime idx 50: Last Exile, Actual = 8.0, Predicted = 7.97\n",
      "Anime idx 100: Higashi no Eden, Actual = 9.0, Predicted = 8.95\n",
      "Anime idx 150: Eikoku Koi Monogatari Emma: Molders-hen, Actual = 7.0, Predicted = 7.13\n",
      "Anime idx 200: Mobile Suit Zeta Gundam, Actual = 6.0, Predicted = 6.06\n",
      "Anime idx 250: Drifters, Actual = 8.0, Predicted = 7.97\n",
      "Anime idx 300: Omae Umasou da na, Actual = 9.0, Predicted = 8.91\n",
      "Anime idx 350: Toaru Kagaku no Railgun S, Actual = 7.0, Predicted = 7.04\n",
      "\n",
      "Top 10 anime recommendations for the new user:\n",
      "Anime ID 33352: Violet Evergarden (Predicted Rating: 11.19)\n",
      "Anime ID 5114: Fullmetal Alchemist: Brotherhood (Predicted Rating: 11.07)\n",
      "Anime ID 1575: Code Geass: Hangyaku no Lelouch (Predicted Rating: 11.01)\n",
      "Anime ID 820: Ginga Eiyuu Densetsu (Predicted Rating: 10.96)\n",
      "Anime ID 25013: Akatsuki no Yona (Predicted Rating: 10.56)\n",
      "Anime ID 37256: Chainsaw Bunny: Deleted Scene (Predicted Rating: 10.40)\n",
      "Anime ID 13601: Psycho-Pass (Predicted Rating: 10.37)\n",
      "Anime ID 3297: Aria The Origination (Predicted Rating: 10.20)\n",
      "Anime ID 33674: No Game No Life: Zero (Predicted Rating: 10.16)\n",
      "Anime ID 31696: Witch Village Story (Predicted Rating: 9.99)\n"
     ]
    }
   ],
   "source": [
    "# Get feature vector and bias for the new user\n",
    "w_new = W[new_user_idx].numpy()   # (num_features,)\n",
    "b_new = b[0, new_user_idx].numpy()  # scalar\n",
    "\n",
    "my_predictions = X.numpy().dot(w_new) + b_new + anime_means_array\n",
    "ix = np.argsort(my_predictions)[::-1]\n",
    "print(\"\\nNew user's actual ratings vs predicted ratings:\")\n",
    "for _, row in new_df.iterrows():\n",
    "    idx = int(row['anime_idx'])\n",
    "    actual = row['score']\n",
    "    predicted = my_predictions[idx]\n",
    "    \n",
    "    # get title from animes_df \n",
    "    title = animes_df['title'].iloc[idx]\n",
    "    \n",
    "    print(f\"Anime idx {idx}: {title}, Actual = {actual}, Predicted = {predicted:.2f}\")\n",
    "print(\"\\nTop 10 anime recommendations for the new user:\")\n",
    "for i in ix[:10]:\n",
    "    anime_id = animes_df['uid'].iloc[i]\n",
    "    title = animes_df['title'].iloc[i]\n",
    "    print(f\"Anime ID {anime_id}: {title} (Predicted Rating: {my_predictions[i]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6c73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b77f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
