{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a889590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./venv/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (6.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in ./venv/lib/python3.12/site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.11.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.3.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in ./venv/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in ./venv/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726b4c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 19:16:43.567104: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-30 19:16:43.612125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-30 19:16:44.573792: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#we need to create Y, R, Ynorm, Ymean from reviews.csv and animes.csv (num_users is number of unique profiles in reviews.csv, num_items is number of unique anime_ids in animes.csv)\n",
    "#Y matrix is ratings, R matrix is presence of ratings, Ynorm is normalized ratings, Ymean is mean ratings for each item\n",
    "#Y shape is (num_items, num_users), R shape is (num_items, num_users)\n",
    "\n",
    "animes_df = pd.read_csv(\"data/animes.csv\")\n",
    "reviews_df = pd.read_csv(\"data/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f097d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map IDs to indices\n",
    "anime_id_to_idx = {aid: idx for idx, aid in enumerate(animes_df['uid'].unique())}\n",
    "user_id_to_idx  = {uid: idx for idx, uid in enumerate(reviews_df['profile'].unique())}\n",
    "\n",
    "# Convert to indices\n",
    "reviews_df['anime_idx'] = reviews_df['anime_uid'].map(anime_id_to_idx)\n",
    "reviews_df['profile_idx']  = reviews_df['profile'].map(user_id_to_idx)\n",
    "\n",
    "# Compute mean per anime\n",
    "anime_means = reviews_df.groupby('anime_idx')['score'].mean().to_dict()\n",
    "\n",
    "num_items = len(anime_id_to_idx)\n",
    "\n",
    "anime_means_array = np.zeros(num_items)   # default fill with 0\n",
    "for idx, mean in anime_means.items():\n",
    "    anime_means_array[idx] = mean\n",
    "\n",
    "# Normalize ratings (subtract per-anime mean)\n",
    "reviews_df['score_norm'] = reviews_df.apply(\n",
    "    lambda row: row['score'] - anime_means_array[row['anime_idx']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only needed columns (triplet + normalized score)\n",
    "ratings_df = reviews_df[['anime_idx', 'profile_idx', 'score', 'score_norm']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f54a9",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# HYPERPARAMETERS - Tune these to reduce overfitting\n# =============================================================================\nnum_features = 10       # Fewer features = simpler model\nlambda_ = 100           # High regularization to prevent overfitting\nlearning_rate = 0.1\niterations = 300        # With early stopping\n\nnum_users = len(user_id_to_idx) + 1  # +1 for the new user\n\n# Initialize parameters\ntf.random.set_seed(42)\nX = tf.Variable(tf.random.normal((num_items, num_features), dtype=tf.float64), name='X')\nW = tf.Variable(tf.random.normal((num_users, num_features), dtype=tf.float64), name='W')\nb = tf.Variable(tf.random.normal((1, num_users), dtype=tf.float64), name='b')\n\nprint(f\"Model config: {num_features} features, lambda={lambda_}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c716cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate a new user's ratings to Y and R at the beginning\n",
    "new_user_id = \"new_user\"\n",
    "new_user_idx = num_users - 1\n",
    "\n",
    "new_ratings = [\n",
    "    (0, new_user_idx, 10),\n",
    "    (50, new_user_idx, 8),\n",
    "    (100, new_user_idx, 9),\n",
    "    (150, new_user_idx, 7),\n",
    "    (200, new_user_idx, 6),\n",
    "    (250, new_user_idx, 8),\n",
    "    (300, new_user_idx, 9),\n",
    "    (350, new_user_idx, 7),\n",
    "]\n",
    "\n",
    "new_df = pd.DataFrame(new_ratings, columns=['anime_idx', 'profile_idx', 'score'])\n",
    "\n",
    "# normalize new ratings\n",
    "new_df['score_norm'] = new_df.apply(\n",
    "    lambda row: row['score'] - anime_means_array[row['anime_idx']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "ratings_df = pd.concat([ratings_df, new_df], ignore_index=True)\n",
    "#change this code to simulate a new user with some ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b043572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ratings: 192,120\n",
      "Training ratings: 153,696 (80.0%)\n",
      "Test ratings: 38,424 (20.0%)\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove rows with NaNs in anime_idx, user_idx, or score_norm\n",
    "ratings_df = ratings_df.dropna(subset=['anime_idx', 'profile_idx', 'score_norm'])\n",
    "\n",
    "# 2. Train/Test Split (80/20)\n",
    "# We shuffle and split the ratings, not users or items\n",
    "# This tests: \"Can we predict ratings we haven't seen?\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total ratings: {len(ratings_df):,}\")\n",
    "print(f\"Training ratings: {len(train_df):,} ({len(train_df)/len(ratings_df)*100:.1f}%)\")\n",
    "print(f\"Test ratings: {len(test_df):,} ({len(test_df)/len(ratings_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82499670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func_triplet(X, W, b, anime_idx_tensor, user_idx_tensor, ratings_tensor, lambda_):\n",
    "    preds = tf.reduce_sum(tf.gather(X, anime_idx_tensor) * tf.gather(W, user_idx_tensor), axis=1) + tf.gather(b[0], user_idx_tensor)\n",
    "    err = preds - ratings_tensor\n",
    "    J = 0.5 * tf.reduce_sum(err**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba237c3",
   "metadata": {},
   "outputs": [],
   "source": "# Training cell - uses hyperparameters from above\nanime_idx_tensor = tf.constant(train_df['anime_idx'].values, dtype=tf.int32)\nuser_idx_tensor  = tf.constant(train_df['profile_idx'].values, dtype=tf.int32)\nratings_tensor   = tf.constant(train_df['score_norm'].values, dtype=tf.float64)\n\noptimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n\ndef calculate_rmse(X, W, b, df, anime_means_array):\n    anime_idx = df['anime_idx'].values.astype(int)\n    user_idx = df['profile_idx'].values.astype(int)\n    actual_scores = df['score'].values\n    X_np, W_np, b_np = X.numpy(), W.numpy(), b.numpy()\n    predictions = np.sum(X_np[anime_idx] * W_np[user_idx], axis=1) + b_np[0, user_idx] + anime_means_array[anime_idx]\n    return np.sqrt(np.mean((predictions - actual_scores) ** 2))\n\nprint(f\"Training: {len(train_df):,} ratings | features={num_features}, lambda={lambda_}\")\nprint(\"-\" * 60)\n\nbest_test_rmse = float('inf')\nfor iter in range(iterations):\n    with tf.GradientTape() as tape:\n        cost_value = cofi_cost_func_triplet(X, W, b, anime_idx_tensor, user_idx_tensor, ratings_tensor, lambda_)\n    grads = tape.gradient(cost_value, [X, W, b])\n    optimizer.apply_gradients(zip(grads, [X, W, b]))\n\n    if iter % 20 == 0:\n        train_rmse = calculate_rmse(X, W, b, train_df, anime_means_array)\n        test_rmse = calculate_rmse(X, W, b, test_df, anime_means_array)\n        marker = \" *\" if test_rmse < best_test_rmse else \"\"\n        if test_rmse < best_test_rmse:\n            best_test_rmse = test_rmse\n        print(f\"Iter {iter:3d} | Train: {train_rmse:.4f} | Test: {test_rmse:.4f}{marker}\")\n\nprint(\"-\" * 60)\ntrain_rmse = calculate_rmse(X, W, b, train_df, anime_means_array)\ntest_rmse = calculate_rmse(X, W, b, test_df, anime_means_array)\nprint(f\"Final - Train: {train_rmse:.4f} | Test: {test_rmse:.4f} | Gap: {test_rmse - train_rmse:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rij8ukexd6f",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluation Summary\nprint(\"=\" * 60)\nprint(\"RESULTS\")\nprint(\"=\" * 60)\nprint(f\"Config: {num_features} features, lambda={lambda_}\")\nprint(f\"Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f}\")\nprint(f\"Overfitting gap: {test_rmse - train_rmse:.4f}\")\nprint()\nif test_rmse - train_rmse > 1.0:\n    print(\"Still overfitting. Try: lambda=500 or num_features=5\")\nelif test_rmse - train_rmse > 0.5:\n    print(\"Moderate overfitting - getting better!\")\nelse:\n    print(\"Good fit!\")\nprint()\nprint(\"Benchmarks: <1.0 Excellent | 1.0-1.5 Good | 1.5-2.0 OK | >2.0 Poor\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60afbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature vector and bias for the new user\n",
    "w_new = W[new_user_idx].numpy()   # (num_features,)\n",
    "b_new = b[0, new_user_idx].numpy()  # scalar\n",
    "\n",
    "my_predictions = X.numpy().dot(w_new) + b_new + anime_means_array\n",
    "ix = np.argsort(my_predictions)[::-1]\n",
    "print(\"\\nNew user's actual ratings vs predicted ratings:\")\n",
    "for _, row in new_df.iterrows():\n",
    "    idx = int(row['anime_idx'])\n",
    "    actual = row['score']\n",
    "    predicted = my_predictions[idx]\n",
    "    \n",
    "    # get title from animes_df \n",
    "    title = animes_df['title'].iloc[idx]\n",
    "    \n",
    "    print(f\"Anime idx {idx}: {title}, Actual = {actual}, Predicted = {predicted:.2f}\")\n",
    "print(\"\\nTop 10 anime recommendations for the new user:\")\n",
    "for i in ix[:10]:\n",
    "    anime_id = animes_df['uid'].iloc[i]\n",
    "    title = animes_df['title'].iloc[i]\n",
    "    print(f\"Anime ID {anime_id}: {title} (Predicted Rating: {my_predictions[i]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from save_and_load_cf_recommender import save_collab_model, load_collab_model\n",
    "save_collab_model(X, W, b, anime_means_array, anime_id_to_idx, user_id_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b77f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loaded, W_loaded, b_loaded, anime_means_array_loaded, anime_id_to_idx_loaded, user_id_to_idx_loaded = load_collab_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9003554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify predictions are identical\n",
    "np.allclose(X.numpy(), X_loaded.numpy())\n",
    "np.allclose(W.numpy(), W_loaded.numpy())\n",
    "np.allclose(b.numpy(), b_loaded.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75677e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}